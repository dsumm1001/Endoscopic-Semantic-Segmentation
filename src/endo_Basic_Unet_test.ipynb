{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsumm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ignite\\handlers\\checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from monai.networks.nets import BasicUNet\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric, MeanIoU, HausdorffDistanceMetric, ConfusionMatrixMetric\n",
    "from monai.transforms import (\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    ToTensord,\n",
    "    RandFlipd,\n",
    "    RandZoomd, \n",
    "    ToTensord, \n",
    "    AsDiscreted,\n",
    "    CenterSpatialCropd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for pytorch compatibility\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class EndoVis2017Dataset(Dataset):\n",
    "    def __init__(self, label_subdir=None, test=False):\n",
    "        self.data = []\n",
    "\n",
    "        if label_subdir is None:\n",
    "            raise ValueError(\"You must specify a `label_subdir` for ground truth masks (e.g., 'instrument_seg_composite').\")\n",
    "\n",
    "        self.root_dir = \"C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/dataset/test/\"\n",
    "        self.label_subdir = label_subdir\n",
    "\n",
    "        # Recursively walk through directory to find left frame image paths and GT image paths\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            if 'left_frames' in subdir:\n",
    "                #print(\"Hit!\")\n",
    "                for file in sorted(files):\n",
    "                    if file.endswith(('.png', '.jpg', '.jpeg')):                     \n",
    "                        img_path = os.path.join(subdir, file)\n",
    "                        #print(img_path)\n",
    "\n",
    "                        gt_root = subdir.replace('left_frames', 'ground_truth')\n",
    "                        mask_path = os.path.join(gt_root, self.label_subdir, file)\n",
    "\n",
    "                        if os.path.exists(mask_path):\n",
    "                            #print(\"Hit!\")\n",
    "                            self.data.append({\"image\": img_path, \"label\": mask_path})    # Dictionary for MONAI compatability\n",
    "\n",
    "        if not test:\n",
    "            transforms_list = [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),                        # Loads image data and metadata from file path dictionaries\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),               # Adjust or add the channel dimension of input data to ensure channel_first shape\n",
    "\n",
    "                # Images are of nominal size 1280x1024 --> resizing for memory efficiency\n",
    "                CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(1024, 1280)),         # Cropping background padding from images\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256, 320)),                  # Imported images are of various sizes: standardize to 320,256\n",
    "\n",
    "                # Apply data augmentation techniqes\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.3, spatial_axis=1),               # Horizontal axis flip imposed w/ 30% prob\n",
    "                #RandRotate90d(keys=[\"image\", \"label\"], prob=0.3, max_k=3),                  # Random 90Â° rotation imposed w/ 30% prob\n",
    "                RandZoomd(keys=[\"image\", \"label\"], prob=0.3, min_zoom=0.75, max_zoom=1.25), # Zoom range (+/-25%) imposed w/ 30% prob\n",
    "                #RandAdjustContrastd(keys=[\"image\"], prob=0.3, gamma=(0.75, 1.25)),          # Contrast variation (+/-25%) imposed w/ 30% prob\n",
    "\n",
    "                ScaleIntensityd(keys=[\"image\"]),                            # Scale the intensity of input image to the value range 0-1\n",
    "                ToTensord(keys=[\"image\", \"label\"]),                         # Ensure data is of tensor type for pytorch usage\n",
    "            ]\n",
    "        else:\n",
    "            transforms_list = [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),                        # Loads image data and metadata from file path dictionaries\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),               # Adjust or add the channel dimension of input data to ensure channel_first shape\n",
    "\n",
    "                # Images are of nominal size 1280x1024 --> resizing for memory efficiency\n",
    "                CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(1024, 1280)),         # Cropping background padding from images\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256, 320)),                  # Imported images are of various sizes: standardize to 320,256\n",
    "\n",
    "                ScaleIntensityd(keys=[\"image\"]),                            # Scale the intensity of input image to the value range 0-1\n",
    "                ToTensord(keys=[\"image\", \"label\"]),                         # Ensure data is of tensor type for pytorch usage\n",
    "            ]\n",
    "        # Additional conditional transforms based on label_subdir\n",
    "        if label_subdir == \"binary_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], threshold=0.5))         # Binary threshold for binary seg\n",
    "        elif label_subdir == \"part_seg_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=5))           # 5 individual class labels for instrument independent part seg\n",
    "        elif label_subdir == \"TypeSegmentation\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=8))            # 8 individual class labels for part independent instrument seg\n",
    "        elif label_subdir == \"instrument_part_seg_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=21))           # 26 individual class labels for instrument & part seg\n",
    "\n",
    "        # Imposing MONAI transforms\n",
    "        # https://docs.monai.io/en/stable/transforms.html\n",
    "        self.transform = Compose(transforms_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns number of imported samples\n",
    "        length = len(self.data)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return transformed sample from the dataset as dictated by the index\n",
    "        sample = self.data[idx]\n",
    "        return self.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MONAIDataLoader(LightningDataModule):\n",
    "    def __init__(self, dataset=None, batch_size: int = None, img_size: int = None, dimensions:int = None):\n",
    "        super().__init__()\n",
    "        if dataset is None:\n",
    "            raise ValueError(\"No dataset given!\")\n",
    "        self.dataset = dataset\n",
    "        self.test_dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = True\n",
    "\n",
    "        self.train, self.val = random_split(self.dataset, [\n",
    "            int(len(self.dataset) * 0.8),\n",
    "            len(self.dataset) - int(len(self.dataset) * 0.8)\n",
    "        ])\n",
    "\n",
    "        print(f\"Train dataset size: {len(self.train)}\")\n",
    "        print(f\"Validation dataset size: {len(self.val)}\")\n",
    "        print(f\"Test dataset size: {len(self.test_dataset)}\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # required by PyTorch Lightning\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_UNet_Train(LightningModule):\n",
    "    def __init__(self, img_size=(1, 3, 256, 320), batch_size=1, lr=0.001, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        print(\"num_classes\", self.num_classes, num_classes, self.hparams.num_classes)\n",
    "        self.example_input_array = [torch.zeros(self.hparams.img_size)]\n",
    "\n",
    "        self.test_step_outputs = []  # Initialize an empty list to store outputs\n",
    "        \n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\", ignore_empty=True)\n",
    "        self.iou_metric = MeanIoU(include_background=True, reduction=\"mean\", ignore_empty=True)\n",
    "        self.hausdorff_metric = HausdorffDistanceMetric(\n",
    "                                                include_background=True,\n",
    "                                                distance_metric=\"euclidean\",\n",
    "                                                percentile=95,\n",
    "                                                directed=False,\n",
    "                                                reduction=\"mean\"\n",
    "                                            )\n",
    "        self.confusion_metric = ConfusionMatrixMetric(\n",
    "            metric_name=[\"precision\", \"recall\", \"f1 score\"],\n",
    "            include_background=False,\n",
    "            compute_sample=False,\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "\n",
    "        # Metric tracking\n",
    "        self.dice_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "        # Defining MONAI Unet model paramters\n",
    "        self.model = BasicUNet(spatial_dims=2,      # 2D image so spatial dims = 2\n",
    "                               in_channels=3,       # RGB input ultrasound image\n",
    "                               out_channels=num_classes,      # Binary segmentation mask output image\n",
    "                               features= (32, 64, 128, 256, 512, 32),        # standard Unet feature sizes (32, 32, 64, 128, 256, 32)\n",
    "                               dropout=0.1)    # Dropout prob 10%\n",
    "        \n",
    "        # Using combined DICE and CE loss as loss function\n",
    "        # Conditional loss function based on the number of classes\n",
    "        if num_classes == 1:\n",
    "            self.DICE_CE_Loss = DiceCELoss(\n",
    "                include_background=False,  # Exclude background class\n",
    "                sigmoid=True,  # Use softmax for multiclass segmentation\n",
    "                softmax=False,  # Apply softmax for multiclass\n",
    "                lambda_dice=1.0,  # Adjust the weight for Dice loss\n",
    "                lambda_ce=1.0,  # Adjust the weight for Cross-Entropy loss\n",
    "                reduction='mean'  # Use mean reduction\n",
    "            )\n",
    "        else:\n",
    "            self.DICE_CE_Loss = DiceCELoss(\n",
    "                include_background=False,  # Exclude background class\n",
    "                sigmoid=False,  # Use softmax for multiclass segmentation\n",
    "                softmax=True,  # Apply softmax for multiclass\n",
    "                lambda_dice=1.0,  # Adjust the weight for Dice loss\n",
    "                lambda_ce=1.0,  # Adjust the weight for Cross-Entropy loss\n",
    "                reduction='mean'  # Use mean reduction\n",
    "            )\n",
    "\n",
    "        # For storing images for the last epoch\n",
    "        self.last_image = []\n",
    "        self.last_pred = []\n",
    "        self.last_mask = []\n",
    "        self.logged_epochs = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Prepare input and ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        if self.hparams.num_classes == 1:\n",
    "            # Binary segmentation\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            gt_input = (gt_input > 0.5).float()\n",
    "\n",
    "        else:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.nn.functional.one_hot(torch.argmax(probs, dim=1), num_classes=self.num_classes)\n",
    "            preds = preds.permute(0, 3, 1, 2).float()  # Shape: [B, C, H, W]\n",
    "\n",
    "        # MONAI metrics\n",
    "        self.dice_metric(y_pred=preds, y=gt_input)\n",
    "        self.iou_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        # Hausdorff: safe only per image if non-empty\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_i = preds[i]\n",
    "            gt_i = gt_input[i]\n",
    "            if torch.any(pred_i) and torch.any(gt_i):  # Check both non-empty\n",
    "                self.hausdorff_metric(y_pred=pred_i.unsqueeze(0), y=gt_i.unsqueeze(0))\n",
    "            else:\n",
    "                print(f\"[Info] Skipping HD metric for empty prediction or GT in batch index {i}\")\n",
    "        #self.hausdorff_metric(y_pred=preds, y=gt_input)\n",
    "        self.confusion_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        # Extract Dice, IoU, Hausdorff from MONAI\n",
    "        # Aggregate & safely handle NaNs\n",
    "        dice = torch.nan_to_num(self.dice_metric.aggregate(), nan=0.0).item()\n",
    "        iou = torch.nan_to_num(self.iou_metric.aggregate(), nan=0.0).item()\n",
    "        hausdorff = torch.nan_to_num(self.hausdorff_metric.aggregate(), nan=0.0).item()\n",
    "        #hausdorff = self.hausdorff_metric.aggregate().item()\n",
    "        #hausdorff = float('nan') if torch.isnan(torch.tensor(hausdorff)) else hausdorff\n",
    "        #hausdorff = torch.nan_to_num(hausdorff, nan=0.0)\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        self.iou_metric.reset()\n",
    "        self.hausdorff_metric.reset()\n",
    "\n",
    "        # Extract precision, recall, f1 score\n",
    "        confusion_metrics = self.confusion_metric.aggregate()\n",
    "        precision, recall, f1 = [m.item() for m in confusion_metrics]\n",
    "        self.confusion_metric.reset()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"test_dice\", dice, prog_bar=True)\n",
    "        self.log(\"test_iou\", iou, prog_bar=True)\n",
    "        self.log(\"test_hausdorff\", hausdorff, prog_bar=True)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "        self.log(\"test_f1\", f1, prog_bar=True)\n",
    "\n",
    "        # Return for aggregation\n",
    "        out = {\n",
    "            \"test_dice\": torch.tensor(dice),\n",
    "            \"test_iou\": torch.tensor(iou),\n",
    "            \"test_precision\": torch.tensor(precision),\n",
    "            \"test_recall\": torch.tensor(recall),\n",
    "            \"test_f1\": torch.tensor(f1),\n",
    "            \"test_hausdorff\": torch.tensor(hausdorff)\n",
    "        }\n",
    "\n",
    "        self.test_step_outputs.append(out)\n",
    "        return out\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Aggregate the results across all batches in the epoch\n",
    "        avg_dice = torch.stack([x[\"test_dice\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_iou = torch.stack([x[\"test_iou\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_hausdorff = torch.stack([x[\"test_hausdorff\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_precision = torch.stack([x[\"test_precision\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_recall = torch.stack([x[\"test_recall\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_f1 = torch.stack([x[\"test_f1\"] for x in self.test_step_outputs]).mean()\n",
    "\n",
    "        print(f\"\\nâ Test Metrics:\"\n",
    "            f\"\\n   Dice       : {avg_dice.item():.4f}\"\n",
    "            f\"\\n   IoU        : {avg_iou.item():.4f}\"\n",
    "            f\"\\n   Hausdorff  : {avg_hausdorff.item():.4f}\"\n",
    "            f\"\\n   Precision  : {avg_precision.item():.4f}\"\n",
    "            f\"\\n   Recall     : {avg_recall.item():.4f}\"\n",
    "            f\"\\n   F1 Score   : {avg_f1.item():.4f}\")\n",
    "\n",
    "        # Clear for next epoch\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def _prepare_batch(self, batch):\n",
    "        return batch['image'], batch['label']\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.DICE_CE_Loss(outputs, gt_input)\n",
    "        self.log(f\"Train_Dice_CE_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        if batch_idx == len(batch) - 1:\n",
    "            self.train_losses.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        # Gets labels for input and corresponding ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.DICE_CE_Loss(outputs, gt_input)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        if self.hparams.num_classes == 1:\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            # Ensure ground truth is binary (i.e., 0 or 1)\n",
    "            gt_input = (gt_input > 0.5).float()  # Threshold the ground truth if needed\n",
    "\n",
    "            intersection = (preds * gt_input).sum()\n",
    "            union = preds.sum() + gt_input.sum()\n",
    "            bin_dice_score = 2.0 * intersection / (union + 1e-8)  # Avoid division by zero\n",
    "            # IoU score calculation for binary segmentation\n",
    "            bin_iou_score = intersection / (union - intersection + 1e-8)  # Avoid division by zero\n",
    "\n",
    "            self.log(\"val_dice\", bin_dice_score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.log(\"val_iou\", bin_iou_score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        else:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.nn.functional.one_hot(torch.argmax(probs, dim=1), num_classes=self.num_classes)\n",
    "            preds = preds.permute(0, 3, 1, 2).float()  # Shape: [B, C, H, W]\n",
    "\n",
    "            self.dice_metric(y_pred=preds, y=gt_input)\n",
    "            self.iou_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        if self.trainer.sanity_checking:\n",
    "            return  # skip logging during sanity check\n",
    "\n",
    "        # Append validation loss at the end of each epoch\n",
    "        if batch_idx == len(batch) - 1:\n",
    "            self.val_losses.append(loss.item())\n",
    "\n",
    "            # For binary segmentation: apply sigmoid and threshold\n",
    "            if self.hparams.num_classes == 1:\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                outputs = (outputs > 0.5).float()  # Convert probabilities to binary mask\n",
    "                self.dice_scores.append(bin_dice_score)\n",
    "                self.iou_scores.append(bin_iou_score)\n",
    "\n",
    "            # For multiclass segmentation: apply softmax\n",
    "            else:\n",
    "                outputs = torch.softmax(outputs, dim=1)  # Apply softmax for multi-class outputs\n",
    "                dice = self.dice_metric.aggregate()[0].item()\n",
    "                #print(\"Dice\", dice)\n",
    "                iou = self.iou_metric.aggregate()[0].item()\n",
    "                #print(\"IOU\", iou)\n",
    "                self.dice_metric.reset()\n",
    "                self.iou_metric.reset()\n",
    "                self.dice_scores.append(dice)\n",
    "                self.iou_scores.append(iou)\n",
    "                self.log(\"val_dice\", dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                self.log(\"val_iou\", iou, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            # Normalize and convert tensor to 3 channels (RGB) for visualization\n",
    "            def process(last):\n",
    "                # Detach from cpu to not interrupt training\n",
    "                # https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor\n",
    "                last = last[0].detach().cpu()\n",
    "\n",
    "                # Min max normalization\n",
    "                # https://www.codecademy.com/article/normalization\n",
    "                last= (last - last.min()) / (last.max() - last.min() + 1e-8)\n",
    "\n",
    "                # If grayscale, reshape last image to RGB for display by replicating gray value twice\n",
    "                # https://discuss.pytorch.org/t/convert-grayscale-images-to-rgb/113422\n",
    "                return last.repeat(3, 1, 1) if last.shape[0] == 1 else last\n",
    "\n",
    "            current_epoch = self.current_epoch\n",
    "            total_epochs = self.trainer.max_epochs\n",
    "            print(\"TE\", total_epochs)\n",
    "\n",
    "            if current_epoch == 0 or current_epoch == total_epochs - 1 or current_epoch == total_epochs // 2:\n",
    "                self.last_image.append(process(inputs))\n",
    "                self.last_pred.append(process(outputs))\n",
    "                self.last_mask.append(process(gt_input))\n",
    "                self.logged_epochs.append(current_epoch)\n",
    "                print(f\"Logged image from epoch {current_epoch}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def plot_losses(self):\n",
    "        min_len = min(len(self.train_losses), len(self.val_losses))\n",
    "        epochs = range(1, min_len + 1)\n",
    "\n",
    "        # Plotting training vs validation loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, self.train_losses[:len(epochs)], label=\"Training Loss\", color='blue')\n",
    "        plt.plot(epochs, self.val_losses[:len(epochs)], label=\"Validation Loss\", color='orange')\n",
    "        plt.title(\"Training vs Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.dice_scores) + 1)\n",
    "\n",
    "        # Convert to CPU floats if necessary\n",
    "        dice = [d.cpu().item() if torch.is_tensor(d) else d for d in self.dice_scores]\n",
    "        iou = [i.cpu().item() if torch.is_tensor(i) else i for i in self.iou_scores]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, dice, label='Dice Coefficient')\n",
    "        plt.plot(epochs, iou, label='IoU')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(\"Validation Metrics Over Time\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_result_by_epoch(self):\n",
    "        total_epochs = len(self.last_image)\n",
    "\n",
    "        if total_epochs < 5:\n",
    "            print(f\"Only {total_epochs} epochs recorded, plotting all.\")\n",
    "            selected_epochs = list(range(total_epochs))\n",
    "        else:\n",
    "            print(f\"{total_epochs} epochs recorded, bug in code.\")\n",
    "\n",
    "        for epoch_idx in selected_epochs:\n",
    "            epoch_num = self.logged_epochs[epoch_idx] if hasattr(self, \"logged_epochs\") else epoch_idx\n",
    "            img = self.last_image[epoch_idx]\n",
    "            pred = self.last_pred[epoch_idx]\n",
    "            mask = self.last_mask[epoch_idx]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "            ax[0].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "            ax[0].set_title(f\"Epoch {epoch_num} - Image\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            if self.hparams.num_classes == 1:\n",
    "                ax[1].imshow(np.transpose(pred.numpy(), (1, 2, 0)))\n",
    "                ax[1].set_title(f\"Epoch {epoch_num} - Prediction\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                ax[2].imshow(np.transpose(mask.numpy(), (1, 2, 0)))\n",
    "                ax[2].set_title(f\"Epoch {epoch_num} - Ground Truth\")\n",
    "                ax[2].axis(\"off\")\n",
    "            else:\n",
    "                # Define the colormap and normalization\n",
    "                num_classes = self.hparams.num_classes\n",
    "                cmap = plt.get_cmap('viridis', num_classes)\n",
    "                bounds = np.arange(num_classes + 1) - 0.5\n",
    "                norm = plt.matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "                # Convert one-hot encoded predictions and masks to single-channel class labels\n",
    "                pred_mask = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "                true_mask = torch.argmax(mask, dim=0).cpu().numpy()\n",
    "\n",
    "                # Apply consistent colormap and normalization\n",
    "                im1 = ax[1].imshow(pred_mask, cmap=cmap, norm=norm)\n",
    "                ax[1].set_title(f\"Epoch {epoch_num} - Prediction\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                im2 = ax[2].imshow(true_mask, cmap=cmap, norm=norm)\n",
    "                ax[2].set_title(f\"Epoch {epoch_num} - Ground Truth\")\n",
    "                ax[2].axis(\"off\")\n",
    "\n",
    "                im_for_cbar = im1  # just need one mappable\n",
    "\n",
    "                # Adjust layout to leave space at the bottom\n",
    "                fig.subplots_adjust(bottom=0.25) # tweak this if labels get cut off\n",
    "\n",
    "                # Add a new axis below the plots for the colorbar\n",
    "                cbar_ax = fig.add_axes([0.1, 0.1, 0.8, 0.10])  # [left, bottom, width, height]\n",
    "                cbar = fig.colorbar(im_for_cbar, cax=cbar_ax, orientation='horizontal', ticks=np.arange(num_classes))\n",
    "\n",
    "                # Add colorbar below the plots\n",
    "                #cbar = fig.colorbar(im1, ax=ax.ravel().tolist(), orientation='horizontal',\n",
    "                        #ticks=np.arange(num_classes), pad=0.15, fraction=0.05)\n",
    "\n",
    "                # Set class labels\n",
    "                if num_classes == 5:\n",
    "                    cbar.ax.set_xticklabels(['Background', 'Shaft', 'Wrist', 'Claspers', 'Probe'])\n",
    "                elif num_classes == 8:\n",
    "                    cbar.ax.set_xticklabels(['Background', 'Bipolar Forceps', 'Prograsp Forceps', 'Large Needle Driver',\n",
    "                                            'Vessel Sealer', 'Grasping Retractor', 'Monopolar Curved Scissors', 'Other'])\n",
    "\n",
    "                    plt.setp(cbar.ax.get_xticklabels(), rotation=30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "                elif num_classes == 21:\n",
    "                    cbar.ax.set_xticklabels([\n",
    "                        \"Background\",\n",
    "                        \"Bipolar Forceps Shaft\", \"Bipolar Forceps Wrist\", \"Bipolar Forceps Claspers\",\n",
    "                        \"Prograsp Forceps Shaft\", \"Prograsp Forceps Wrist\", \"Prograsp Forceps Claspers\",\n",
    "                        \"Large Needle Driver Shaft\", \"Large Needle Driver Wrist\", \"Large Needle Driver Claspers\",\n",
    "                        \"Vessel Sealer Shaft\", \"Vessel Sealer Wrist\", \"Vessel Sealer Claspers\",\n",
    "                        \"Grasping Retractor Shaft\", \"Grasping Retractor Wrist\", \"Grasping Retractor Claspers\",\n",
    "                        \"Monopolar Curved Scissors Shaft\", \"Monopolar Curved Scissors Wrist\", \"Monopolar Curved Scissors Claspers\",\n",
    "                        \"Other Probe\",\"Other Probe\"\n",
    "                    ])\n",
    "                    plt.setp(cbar.ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        \n",
    "                cbar.set_label('Class ID')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 1 1 1\n",
      "BasicUNet features: (32, 64, 128, 256, 512, 32).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc32e404944407f80e16c829c52ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â Test Metrics:\n",
      "   Dice       : 0.9100\n",
      "   IoU        : 0.8552\n",
      "   Hausdorff  : 28.7077\n",
      "   Precision  : 0.8954\n",
      "   Recall     : 0.9532\n",
      "   F1 Score   : 0.9139\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\">        Test metric        </span>â<span style=\"font-weight: bold\">       DataLoader 0        </span>â\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.9099989533424377     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.9138563275337219     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    28.707733154296875     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.8551986813545227     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">     0.895444393157959     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.9531691670417786     </span>â\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.9099989533424377    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.9138563275337219    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   28.707733154296875    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.8551986813545227    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m    0.895444393157959    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.9531691670417786    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.9099989533424377,\n",
       "  'test_iou': 0.8551986813545227,\n",
       "  'test_hausdorff': 28.707733154296875,\n",
       "  'test_precision': 0.895444393157959,\n",
       "  'test_recall': 0.9531691670417786,\n",
       "  'test_f1': 0.9138563275337219}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_basic_UNet_model = basic_UNet_Train(num_classes=1)\n",
    "binary_basic_UNet_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/Basic_UNet/basicUNetmodels/binary_basic_UNet_model.pth'))\n",
    "\n",
    "binary_endo_images = EndoVis2017Dataset(label_subdir='binarySegmentation', test=True)\n",
    "binary_endo_data = MONAIDataLoader(dataset=binary_endo_images, batch_size=20)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=binary_basic_UNet_model, datamodule=binary_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.007365 seconds\n"
     ]
    }
   ],
   "source": [
    "binary_basic_UNet_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 10  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(binary_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = binary_basic_UNet_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 5 5 5\n",
      "BasicUNet features: (32, 64, 128, 256, 512, 32).\n",
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd660a26322a4f62ad233eb674db7f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the ground truth of class 4 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 4 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 2 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 3 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 2 is all 0, this may result in nan/inf distance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â Test Metrics:\n",
      "   Dice       : 0.7990\n",
      "   IoU        : 0.7129\n",
      "   Hausdorff  : 43.1655\n",
      "   Precision  : 0.7664\n",
      "   Recall     : 0.8284\n",
      "   F1 Score   : 0.7872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\">        Test metric        </span>â<span style=\"font-weight: bold\">       DataLoader 0        </span>â\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>â<span style=\"color: #800080; text-decoration-color: #800080\">     0.799024224281311     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">     0.787174642086029     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    43.165470123291016     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.7128857374191284     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.7663677334785461     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.8283786773681641     </span>â\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m    0.799024224281311    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m    0.787174642086029    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   43.165470123291016    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.7128857374191284    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.7663677334785461    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.8283786773681641    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.799024224281311,\n",
       "  'test_iou': 0.7128857374191284,\n",
       "  'test_hausdorff': 43.165470123291016,\n",
       "  'test_precision': 0.7663677334785461,\n",
       "  'test_recall': 0.8283786773681641,\n",
       "  'test_f1': 0.787174642086029}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_seg_basic_UNet_model = basic_UNet_Train(num_classes=5)\n",
    "part_seg_basic_UNet_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/Basic_UNet/basicUNetmodels/part_seg_basic_UNet_model.pth'))\n",
    "\n",
    "part_seg_endo_images = EndoVis2017Dataset(label_subdir='part_seg_composite', test=True)\n",
    "part_seg_endo_data = MONAIDataLoader(dataset=part_seg_endo_images, batch_size=10)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=part_seg_basic_UNet_model, datamodule=part_seg_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.007161 seconds\n"
     ]
    }
   ],
   "source": [
    "part_seg_basic_UNet_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 20  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(part_seg_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = part_seg_basic_UNet_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 8 8 8\n",
      "BasicUNet features: (32, 64, 128, 256, 512, 32).\n",
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc9f316b094db0a75703293fbd9705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the ground truth of class 5 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 6 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 7 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 6 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 5 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 7 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 1 is all 0, this may result in nan/inf distance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â Test Metrics:\n",
      "   Dice       : 0.5215\n",
      "   IoU        : 0.4717\n",
      "   Hausdorff  : 47.7621\n",
      "   Precision  : 0.6344\n",
      "   Recall     : 0.6564\n",
      "   F1 Score   : 0.6392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\">        Test metric        </span>â<span style=\"font-weight: bold\">       DataLoader 0        </span>â\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.5215451717376709     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.6391516327857971     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">     47.76210021972656     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.4717252850532532     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.6343963146209717     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.6564075350761414     </span>â\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.5215451717376709    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.6391516327857971    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m    47.76210021972656    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.4717252850532532    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.6343963146209717    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.6564075350761414    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.5215451717376709,\n",
       "  'test_iou': 0.4717252850532532,\n",
       "  'test_hausdorff': 47.76210021972656,\n",
       "  'test_precision': 0.6343963146209717,\n",
       "  'test_recall': 0.6564075350761414,\n",
       "  'test_f1': 0.6391516327857971}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_seg_basic_UNet_model = basic_UNet_Train(num_classes=8)\n",
    "instr_seg_basic_UNet_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/Basic_UNet/basicUNetmodels/instr_seg_basic_UNet_model.pth'))\n",
    "\n",
    "instr_seg_endo_images = EndoVis2017Dataset(label_subdir='TypeSegmentation', test=True)\n",
    "instr_seg_endo_data = MONAIDataLoader(dataset=instr_seg_endo_images, batch_size=5)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=instr_seg_basic_UNet_model, datamodule=instr_seg_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.248426 seconds\n"
     ]
    }
   ],
   "source": [
    "instr_seg_basic_UNet_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 40  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(instr_seg_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = instr_seg_basic_UNet_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
