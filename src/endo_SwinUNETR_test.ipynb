{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsumm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ignite\\handlers\\checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric, MeanIoU, HausdorffDistanceMetric, ConfusionMatrixMetric\n",
    "from monai.transforms import (\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    ToTensord,\n",
    "    RandFlipd,\n",
    "    RandZoomd, \n",
    "    ToTensord, \n",
    "    AsDiscreted,\n",
    "    CenterSpatialCropd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for pytorch compatibility\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class EndoVis2017Dataset(Dataset):\n",
    "    def __init__(self, label_subdir=None, test=False):\n",
    "        self.data = []\n",
    "\n",
    "        if label_subdir is None:\n",
    "            raise ValueError(\"You must specify a `label_subdir` for ground truth masks (e.g., 'instrument_seg_composite').\")\n",
    "\n",
    "        self.root_dir = \"C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/dataset/test/\"\n",
    "        self.label_subdir = label_subdir\n",
    "\n",
    "        # Recursively walk through directory to find left frame image paths and GT image paths\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            if 'left_frames' in subdir:\n",
    "                #print(\"Hit!\")\n",
    "                for file in sorted(files):\n",
    "                    if file.endswith(('.png', '.jpg', '.jpeg')):                     \n",
    "                        img_path = os.path.join(subdir, file)\n",
    "                        #print(img_path)\n",
    "\n",
    "                        gt_root = subdir.replace('left_frames', 'ground_truth')\n",
    "                        mask_path = os.path.join(gt_root, self.label_subdir, file)\n",
    "\n",
    "                        if os.path.exists(mask_path):\n",
    "                            #print(\"Hit!\")\n",
    "                            self.data.append({\"image\": img_path, \"label\": mask_path})    # Dictionary for MONAI compatability\n",
    "\n",
    "        if not test:\n",
    "            transforms_list = [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),                        # Loads image data and metadata from file path dictionaries\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),               # Adjust or add the channel dimension of input data to ensure channel_first shape\n",
    "\n",
    "                # Images are of nominal size 1280x1024 --> resizing for memory efficiency\n",
    "                CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(1024, 1280)),         # Cropping background padding from images\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256, 320)),                  # Imported images are of various sizes: standardize to 320,256\n",
    "\n",
    "                # Apply data augmentation techniqes\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.3, spatial_axis=1),               # Horizontal axis flip imposed w/ 30% prob\n",
    "                #RandRotate90d(keys=[\"image\", \"label\"], prob=0.3, max_k=3),                  # Random 90° rotation imposed w/ 30% prob\n",
    "                RandZoomd(keys=[\"image\", \"label\"], prob=0.3, min_zoom=0.75, max_zoom=1.25), # Zoom range (+/-25%) imposed w/ 30% prob\n",
    "                #RandAdjustContrastd(keys=[\"image\"], prob=0.3, gamma=(0.75, 1.25)),          # Contrast variation (+/-25%) imposed w/ 30% prob\n",
    "\n",
    "                ScaleIntensityd(keys=[\"image\"]),                            # Scale the intensity of input image to the value range 0-1\n",
    "                ToTensord(keys=[\"image\", \"label\"]),                         # Ensure data is of tensor type for pytorch usage\n",
    "            ]\n",
    "        else:\n",
    "            transforms_list = [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),                        # Loads image data and metadata from file path dictionaries\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),               # Adjust or add the channel dimension of input data to ensure channel_first shape\n",
    "\n",
    "                # Images are of nominal size 1280x1024 --> resizing for memory efficiency\n",
    "                CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(1024, 1280)),         # Cropping background padding from images\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256, 320)),                  # Imported images are of various sizes: standardize to 320,256\n",
    "\n",
    "                ScaleIntensityd(keys=[\"image\"]),                            # Scale the intensity of input image to the value range 0-1\n",
    "                ToTensord(keys=[\"image\", \"label\"]),                         # Ensure data is of tensor type for pytorch usage\n",
    "            ]\n",
    "        # Additional conditional transforms based on label_subdir\n",
    "        if label_subdir == \"binary_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], threshold=0.5))         # Binary threshold for binary seg\n",
    "        elif label_subdir == \"part_seg_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=5))           # 5 individual class labels for instrument independent part seg\n",
    "        elif label_subdir == \"TypeSegmentation\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=8))            # 8 individual class labels for part independent instrument seg\n",
    "        elif label_subdir == \"instrument_part_seg_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=21))           # 26 individual class labels for instrument & part seg\n",
    "\n",
    "        # Imposing MONAI transforms\n",
    "        # https://docs.monai.io/en/stable/transforms.html\n",
    "        self.transform = Compose(transforms_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns number of imported samples\n",
    "        length = len(self.data)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return transformed sample from the dataset as dictated by the index\n",
    "        sample = self.data[idx]\n",
    "        return self.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MONAIDataLoader(LightningDataModule):\n",
    "    def __init__(self, dataset=None, batch_size: int = None, img_size: int = None, dimensions:int = None):\n",
    "        super().__init__()\n",
    "        if dataset is None:\n",
    "            raise ValueError(\"No dataset given!\")\n",
    "        self.dataset = dataset\n",
    "        self.test_dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = True\n",
    "\n",
    "        self.train, self.val = random_split(self.dataset, [\n",
    "            int(len(self.dataset) * 0.8),\n",
    "            len(self.dataset) - int(len(self.dataset) * 0.8)\n",
    "        ])\n",
    "\n",
    "        print(f\"Train dataset size: {len(self.train)}\")\n",
    "        print(f\"Validation dataset size: {len(self.val)}\")\n",
    "        print(f\"Test dataset size: {len(self.test_dataset)}\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # required by PyTorch Lightning\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinUNETR_Train(LightningModule):\n",
    "    def __init__(self, img_size=(1, 3, 256, 320), batch_size=1, lr=0.001, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        print(\"num_classes\", self.num_classes, num_classes, self.hparams.num_classes)\n",
    "        self.example_input_array = [torch.zeros(self.hparams.img_size)]\n",
    "\n",
    "        self.test_step_outputs = []  # Initialize an empty list to store outputs\n",
    "\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\", ignore_empty=True)\n",
    "        self.iou_metric = MeanIoU(include_background=True, reduction=\"mean\", ignore_empty=True)\n",
    "        self.hausdorff_metric = HausdorffDistanceMetric(\n",
    "                                                include_background=True,\n",
    "                                                distance_metric=\"euclidean\",\n",
    "                                                percentile=95,\n",
    "                                                directed=False,\n",
    "                                                reduction=\"mean\"\n",
    "                                            )\n",
    "        self.confusion_metric = ConfusionMatrixMetric(\n",
    "            metric_name=[\"precision\", \"recall\", \"f1 score\"],\n",
    "            include_background=False,\n",
    "            compute_sample=False,\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "\n",
    "        # Metric tracking\n",
    "        self.dice_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "        # Define SwinUNETR model from MONAI\n",
    "        self.model = SwinUNETR(\n",
    "            img_size=(256,320),\n",
    "            in_channels=3,\n",
    "            out_channels=self.num_classes,\n",
    "            feature_size=48,                        # common starting point; can increase to 96/128\n",
    "            drop_rate=0.1,                          # 10% dropout probability\n",
    "            use_checkpoint=True,                     # Enable gradient checkpointing to save memory\n",
    "            spatial_dims = 2,\n",
    "        )\n",
    "\n",
    "        # Using combined DICE and CE loss as loss function\n",
    "        # Conditional loss function based on the number of classes\n",
    "        if num_classes == 1:\n",
    "            self.DICE_CE_Loss = DiceCELoss(\n",
    "                include_background=False,  # Exclude background class\n",
    "                sigmoid=True,  # Use softmax for multiclass segmentation\n",
    "                softmax=False,  # Apply softmax for multiclass\n",
    "                lambda_dice=1.0,  # Adjust the weight for Dice loss\n",
    "                lambda_ce=1.0,  # Adjust the weight for Cross-Entropy loss\n",
    "                reduction='mean'  # Use mean reduction\n",
    "            )\n",
    "        else:\n",
    "            self.DICE_CE_Loss = DiceCELoss(\n",
    "                include_background=False,  # Exclude background class\n",
    "                sigmoid=False,  # Use softmax for multiclass segmentation\n",
    "                softmax=True,  # Apply softmax for multiclass\n",
    "                lambda_dice=1.0,  # Adjust the weight for Dice loss\n",
    "                lambda_ce=1.0,  # Adjust the weight for Cross-Entropy loss\n",
    "                reduction='mean'  # Use mean reduction\n",
    "            )\n",
    "\n",
    "        # Tracking losses for matplotlib\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        # For storing images for the last epoch\n",
    "        self.last_image = []\n",
    "        self.last_pred = []\n",
    "        self.last_mask = []\n",
    "        self.logged_epochs = []\n",
    "\n",
    "    # Passes model inputs through U-net to get output predictions\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Prepare input and ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        if self.hparams.num_classes == 1:\n",
    "            # Binary segmentation\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            gt_input = (gt_input > 0.5).float()\n",
    "\n",
    "        else:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.nn.functional.one_hot(torch.argmax(probs, dim=1), num_classes=self.num_classes)\n",
    "            preds = preds.permute(0, 3, 1, 2).float()  # Shape: [B, C, H, W]\n",
    "\n",
    "        # MONAI metrics\n",
    "        self.dice_metric(y_pred=preds, y=gt_input)\n",
    "        self.iou_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        # Hausdorff: safe only per image if non-empty\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_i = preds[i]\n",
    "            gt_i = gt_input[i]\n",
    "            if torch.any(pred_i) and torch.any(gt_i):  # Check both non-empty\n",
    "                self.hausdorff_metric(y_pred=pred_i.unsqueeze(0), y=gt_i.unsqueeze(0))\n",
    "            else:\n",
    "                print(f\"[Info] Skipping HD metric for empty prediction or GT in batch index {i}\")\n",
    "        #self.hausdorff_metric(y_pred=preds, y=gt_input)\n",
    "        self.confusion_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        # Extract Dice, IoU, Hausdorff from MONAI\n",
    "        # Aggregate & safely handle NaNs\n",
    "        dice = torch.nan_to_num(self.dice_metric.aggregate(), nan=0.0).item()\n",
    "        iou = torch.nan_to_num(self.iou_metric.aggregate(), nan=0.0).item()\n",
    "        hausdorff = torch.nan_to_num(self.hausdorff_metric.aggregate(), nan=0.0).item()\n",
    "        #hausdorff = self.hausdorff_metric.aggregate().item()\n",
    "        #hausdorff = float('nan') if torch.isnan(torch.tensor(hausdorff)) else hausdorff\n",
    "        #hausdorff = torch.nan_to_num(hausdorff, nan=0.0)\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        self.iou_metric.reset()\n",
    "        self.hausdorff_metric.reset()\n",
    "\n",
    "        # Extract precision, recall, f1 score\n",
    "        confusion_metrics = self.confusion_metric.aggregate()\n",
    "        precision, recall, f1 = [m.item() for m in confusion_metrics]\n",
    "        self.confusion_metric.reset()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"test_dice\", dice, prog_bar=True)\n",
    "        self.log(\"test_iou\", iou, prog_bar=True)\n",
    "        self.log(\"test_hausdorff\", hausdorff, prog_bar=True)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "        self.log(\"test_f1\", f1, prog_bar=True)\n",
    "\n",
    "        # Return for aggregation\n",
    "        out = {\n",
    "            \"test_dice\": torch.tensor(dice),\n",
    "            \"test_iou\": torch.tensor(iou),\n",
    "            \"test_precision\": torch.tensor(precision),\n",
    "            \"test_recall\": torch.tensor(recall),\n",
    "            \"test_f1\": torch.tensor(f1),\n",
    "            \"test_hausdorff\": torch.tensor(hausdorff)\n",
    "        }\n",
    "\n",
    "        self.test_step_outputs.append(out)\n",
    "        return out\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Aggregate the results across all batches in the epoch\n",
    "        avg_dice = torch.stack([x[\"test_dice\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_iou = torch.stack([x[\"test_iou\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_hausdorff = torch.stack([x[\"test_hausdorff\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_precision = torch.stack([x[\"test_precision\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_recall = torch.stack([x[\"test_recall\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_f1 = torch.stack([x[\"test_f1\"] for x in self.test_step_outputs]).mean()\n",
    "\n",
    "        print(f\"\\n✅ Test Metrics:\"\n",
    "            f\"\\n   Dice       : {avg_dice.item():.4f}\"\n",
    "            f\"\\n   IoU        : {avg_iou.item():.4f}\"\n",
    "            f\"\\n   Hausdorff  : {avg_hausdorff.item():.4f}\"\n",
    "            f\"\\n   Precision  : {avg_precision.item():.4f}\"\n",
    "            f\"\\n   Recall     : {avg_recall.item():.4f}\"\n",
    "            f\"\\n   F1 Score   : {avg_f1.item():.4f}\")\n",
    "\n",
    "        # Clear for next epoch\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Gets labels for input and corresponding ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "\n",
    "        # Call forward pass\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        # Compute DICE & CE loss based on current params\n",
    "        loss = self.DICE_CE_Loss(outputs, gt_input)\n",
    "\n",
    "        # Log DICE loss with PyTorch Lightning logger\n",
    "        self.log(f\"Train_Dice_CE_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Append train loss at the end of each epoch\n",
    "        if batch_idx == len(batch) - 1:\n",
    "            self.train_losses.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        # Gets labels for input and corresponding ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.DICE_CE_Loss(outputs, gt_input)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        if self.hparams.num_classes == 1:\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            # Ensure ground truth is binary (i.e., 0 or 1)\n",
    "            gt_input = (gt_input > 0.5).float()  # Threshold the ground truth if needed\n",
    "\n",
    "            intersection = (preds * gt_input).sum()\n",
    "            union = preds.sum() + gt_input.sum()\n",
    "            bin_dice_score = 2.0 * intersection / (union + 1e-8)  # Avoid division by zero\n",
    "            # IoU score calculation for binary segmentation\n",
    "            bin_iou_score = intersection / (union - intersection + 1e-8)  # Avoid division by zero\n",
    "\n",
    "            self.log(\"val_dice\", bin_dice_score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.log(\"val_iou\", bin_iou_score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        else:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.nn.functional.one_hot(torch.argmax(probs, dim=1), num_classes=self.num_classes)\n",
    "            preds = preds.permute(0, 3, 1, 2).float()  # Shape: [B, C, H, W]\n",
    "\n",
    "            self.dice_metric(y_pred=preds, y=gt_input)\n",
    "            self.iou_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        if self.trainer.sanity_checking:\n",
    "            return  # skip logging during sanity check\n",
    "\n",
    "        # Append validation loss at the end of each epoch\n",
    "        if batch_idx == len(batch) - 1:\n",
    "            self.val_losses.append(loss.item())\n",
    "\n",
    "            # For binary segmentation: apply sigmoid and threshold\n",
    "            if self.hparams.num_classes == 1:\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                outputs = (outputs > 0.5).float()  # Convert probabilities to binary mask\n",
    "                self.dice_scores.append(bin_dice_score)\n",
    "                self.iou_scores.append(bin_iou_score)\n",
    "\n",
    "            # For multiclass segmentation: apply softmax\n",
    "            else:\n",
    "                outputs = torch.softmax(outputs, dim=1)  # Apply softmax for multi-class outputs\n",
    "                dice = self.dice_metric.aggregate()[0].item()\n",
    "                #print(\"Dice\", dice)\n",
    "                iou = self.iou_metric.aggregate()[0].item()\n",
    "                #print(\"IOU\", iou)\n",
    "                self.dice_metric.reset()\n",
    "                self.iou_metric.reset()\n",
    "                self.dice_scores.append(dice)\n",
    "                self.iou_scores.append(iou)\n",
    "                self.log(\"val_dice\", dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                self.log(\"val_iou\", iou, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            # Normalize and convert tensor to 3 channels (RGB) for visualization\n",
    "            def process(last):\n",
    "                # Detach from cpu to not interrupt training\n",
    "                # https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor\n",
    "                last = last[0].detach().cpu()\n",
    "\n",
    "                # Min max normalization\n",
    "                # https://www.codecademy.com/article/normalization\n",
    "                last= (last - last.min()) / (last.max() - last.min() + 1e-8)\n",
    "\n",
    "                # If grayscale, reshape last image to RGB for display by replicating gray value twice\n",
    "                # https://discuss.pytorch.org/t/convert-grayscale-images-to-rgb/113422\n",
    "                return last.repeat(3, 1, 1) if last.shape[0] == 1 else last\n",
    "\n",
    "            current_epoch = self.current_epoch\n",
    "            total_epochs = self.trainer.max_epochs\n",
    "            #print(\"TE\", total_epochs)\n",
    "\n",
    "            if current_epoch == 0 or current_epoch == total_epochs - 1 or current_epoch == total_epochs // 2:\n",
    "                self.last_image.append(process(inputs))\n",
    "                self.last_pred.append(process(outputs))\n",
    "                self.last_mask.append(process(gt_input))\n",
    "                self.logged_epochs.append(current_epoch)\n",
    "                print(f\"Logged image from epoch {current_epoch}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "    #    return self(batch['image'])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #set optimizer\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # halve LR every 5 epochs\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _prepare_batch(self, batch):\n",
    "        return batch['image'], batch['label']\n",
    "\n",
    "    # Plot training and val losses when needed\n",
    "    def plot_losses(self):\n",
    "        min_len = min(len(self.train_losses), len(self.val_losses))\n",
    "        epochs = range(1, min_len + 1)\n",
    "\n",
    "        # Plotting training vs validation loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, self.train_losses[:len(epochs)], label=\"Training Loss\", color='blue')\n",
    "        plt.plot(epochs, self.val_losses[:len(epochs)], label=\"Validation Loss\", color='orange')\n",
    "        plt.title(\"Training vs Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.dice_scores) + 1)\n",
    "\n",
    "        # Convert to CPU floats if necessary\n",
    "        dice = [d.cpu().item() if torch.is_tensor(d) else d for d in self.dice_scores]\n",
    "        iou = [i.cpu().item() if torch.is_tensor(i) else i for i in self.iou_scores]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, dice, label='Dice Coefficient')\n",
    "        plt.plot(epochs, iou, label='IoU')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(\"Validation Metrics Over Time\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_result_by_epoch(self):\n",
    "        total_epochs = len(self.last_image)\n",
    "        print(\"Total Epochs:\", total_epochs)\n",
    "\n",
    "        if total_epochs < 5:\n",
    "            print(f\"Only {total_epochs} epochs recorded, plotting all.\")\n",
    "            selected_epochs = list(range(total_epochs))\n",
    "        else:\n",
    "            print(f\"{total_epochs} epochs recorded, bug in code.\")\n",
    "\n",
    "        for epoch_idx in selected_epochs:\n",
    "            epoch_num = self.logged_epochs[epoch_idx] if hasattr(self, \"logged_epochs\") else epoch_idx\n",
    "            img = self.last_image[epoch_idx]\n",
    "            pred = self.last_pred[epoch_idx]\n",
    "            mask = self.last_mask[epoch_idx]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "            ax[0].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "            ax[0].set_title(f\"Epoch {epoch_num} - Image\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            if self.hparams.num_classes == 1:\n",
    "                ax[1].imshow(np.transpose(pred.numpy(), (1, 2, 0)))\n",
    "                ax[1].set_title(f\"Epoch {epoch_num} - Prediction\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                ax[2].imshow(np.transpose(mask.numpy(), (1, 2, 0)))\n",
    "                ax[2].set_title(f\"Epoch {epoch_num} - Ground Truth\")\n",
    "                ax[2].axis(\"off\")\n",
    "            else:\n",
    "                # Define the colormap and normalization\n",
    "                num_classes = self.hparams.num_classes\n",
    "                cmap = plt.get_cmap('viridis', num_classes)\n",
    "                bounds = np.arange(num_classes + 1) - 0.5\n",
    "                norm = plt.matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "                # Convert one-hot encoded predictions and masks to single-channel class labels\n",
    "                pred_mask = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "                true_mask = torch.argmax(mask, dim=0).cpu().numpy()\n",
    "\n",
    "                # Apply consistent colormap and normalization\n",
    "                im1 = ax[1].imshow(pred_mask, cmap=cmap, norm=norm)\n",
    "                ax[1].set_title(f\"Epoch {epoch_num} - Prediction\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                im2 = ax[2].imshow(true_mask, cmap=cmap, norm=norm)\n",
    "                ax[2].set_title(f\"Epoch {epoch_num} - Ground Truth\")\n",
    "                ax[2].axis(\"off\")\n",
    "\n",
    "                im_for_cbar = im1  # just need one mappable\n",
    "\n",
    "                # Adjust layout to leave space at the bottom\n",
    "                fig.subplots_adjust(bottom=0.25) # tweak this if labels get cut off\n",
    "\n",
    "                # Add a new axis below the plots for the colorbar\n",
    "                cbar_ax = fig.add_axes([0.1, 0.1, 0.8, 0.10])  # [left, bottom, width, height]\n",
    "                cbar = fig.colorbar(im_for_cbar, cax=cbar_ax, orientation='horizontal', ticks=np.arange(num_classes))\n",
    "\n",
    "                # Set class labels\n",
    "                if num_classes == 5:\n",
    "                    cbar.ax.set_xticklabels(['Background', 'Shaft', 'Wrist', 'Claspers', 'Probe'])\n",
    "                elif num_classes == 8:\n",
    "                    cbar.ax.set_xticklabels(['Background', 'Bipolar Forceps', 'Prograsp Forceps', 'Large Needle Driver',\n",
    "                                            'Vessel Sealer', 'Grasping Retractor', 'Monopolar Curved Scissors', 'Other'])\n",
    "\n",
    "                    plt.setp(cbar.ax.get_xticklabels(), rotation=30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "                elif num_classes == 21:\n",
    "                    cbar.ax.set_xticklabels([\n",
    "                        \"Background\",\n",
    "                        \"Bipolar Forceps Shaft\", \"Bipolar Forceps Wrist\", \"Bipolar Forceps Claspers\",\n",
    "                        \"Prograsp Forceps Shaft\", \"Prograsp Forceps Wrist\", \"Prograsp Forceps Claspers\",\n",
    "                        \"Large Needle Driver Shaft\", \"Large Needle Driver Wrist\", \"Large Needle Driver Claspers\",\n",
    "                        \"Vessel Sealer Shaft\", \"Vessel Sealer Wrist\", \"Vessel Sealer Claspers\",\n",
    "                        \"Grasping Retractor Shaft\", \"Grasping Retractor Wrist\", \"Grasping Retractor Claspers\",\n",
    "                        \"Monopolar Curved Scissors Shaft\", \"Monopolar Curved Scissors Wrist\", \"Monopolar Curved Scissors Claspers\",\n",
    "                        \"Other Probe\",\"Other Probe\"\n",
    "                    ])\n",
    "                    plt.setp(cbar.ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        \n",
    "                cbar.set_label('Class ID')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8512275cf1c43deb5f350ab0fd30d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test Metrics:\n",
      "   Dice       : 0.9156\n",
      "   IoU        : 0.8632\n",
      "   Hausdorff  : 24.0247\n",
      "   Precision  : 0.9038\n",
      "   Recall     : 0.9528\n",
      "   F1 Score   : 0.9187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9156012535095215     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9186905026435852     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    24.024734497070312     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8631661534309387     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9038184285163879     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9528173804283142     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9156012535095215    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9186905026435852    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   24.024734497070312    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8631661534309387    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9038184285163879    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9528173804283142    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.9156012535095215,\n",
       "  'test_iou': 0.8631661534309387,\n",
       "  'test_hausdorff': 24.024734497070312,\n",
       "  'test_precision': 0.9038184285163879,\n",
       "  'test_recall': 0.9528173804283142,\n",
       "  'test_f1': 0.9186905026435852}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_SwinUNETR_model = SwinUNETR_Train(num_classes=1)\n",
    "binary_SwinUNETR_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/SwinUNETR/SwinUNETRmodels/binary_SwinUNETR_model.pth'))\n",
    "\n",
    "binary_endo_images = EndoVis2017Dataset(label_subdir='binarySegmentation', test=True)\n",
    "binary_endo_data = MONAIDataLoader(dataset=binary_endo_images, batch_size=20)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=binary_SwinUNETR_model, datamodule=binary_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.015349 seconds\n"
     ]
    }
   ],
   "source": [
    "binary_SwinUNETR_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 10  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(binary_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = binary_SwinUNETR_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 5 5 5\n",
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cf96c921e44f3d9f609aa38fbb6b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the ground truth of class 4 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 4 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 2 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 2 is all 0, this may result in nan/inf distance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test Metrics:\n",
      "   Dice       : 0.7915\n",
      "   IoU        : 0.7053\n",
      "   Hausdorff  : 41.3787\n",
      "   Precision  : 0.7683\n",
      "   Recall     : 0.8137\n",
      "   F1 Score   : 0.7826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7915090322494507     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7826336026191711     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    41.378692626953125     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7053414583206177     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7682565450668335     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8136885166168213     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7915090322494507    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7826336026191711    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   41.378692626953125    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7053414583206177    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7682565450668335    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8136885166168213    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.7915090322494507,\n",
       "  'test_iou': 0.7053414583206177,\n",
       "  'test_hausdorff': 41.378692626953125,\n",
       "  'test_precision': 0.7682565450668335,\n",
       "  'test_recall': 0.8136885166168213,\n",
       "  'test_f1': 0.7826336026191711}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_seg_SwinUNETR_model = SwinUNETR_Train(num_classes=5)\n",
    "part_seg_SwinUNETR_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/SwinUNETR/SwinUNETRmodels/part_seg_SwinUNETR_model.pth'))\n",
    "\n",
    "part_seg_endo_images = EndoVis2017Dataset(label_subdir='part_seg_composite', test=True)\n",
    "part_seg_endo_data = MONAIDataLoader(dataset=part_seg_endo_images, batch_size=10)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=part_seg_SwinUNETR_model, datamodule=part_seg_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.014753 seconds\n"
     ]
    }
   ],
   "source": [
    "part_seg_SwinUNETR_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 20  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(part_seg_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = part_seg_SwinUNETR_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 8 8 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea36e8b7e66b474cbc32c53b3e9fc6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the ground truth of class 5 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 6 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 7 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 5 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 6 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 7 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 1 is all 0, this may result in nan/inf distance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test Metrics:\n",
      "   Dice       : 0.5318\n",
      "   IoU        : 0.4814\n",
      "   Hausdorff  : 38.7343\n",
      "   Precision  : 0.6533\n",
      "   Recall     : 0.6854\n",
      "   F1 Score   : 0.6613\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5317918062210083     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6613436341285706     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     38.73433303833008     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.481431782245636     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6533302068710327     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6854087710380554     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5317918062210083    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6613436341285706    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    38.73433303833008    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.481431782245636    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6533302068710327    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6854087710380554    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.5317918062210083,\n",
       "  'test_iou': 0.481431782245636,\n",
       "  'test_hausdorff': 38.73433303833008,\n",
       "  'test_precision': 0.6533302068710327,\n",
       "  'test_recall': 0.6854087710380554,\n",
       "  'test_f1': 0.6613436341285706}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_seg_SwinUNETR_model = SwinUNETR_Train(num_classes=8)\n",
    "instr_seg_SwinUNETR_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/SwinUNETR/SwinUNETRmodels/instr_seg_SwinUNETR_model.pth'))\n",
    "\n",
    "instr_seg_endo_images = EndoVis2017Dataset(label_subdir='TypeSegmentation', test=True)\n",
    "instr_seg_endo_data = MONAIDataLoader(dataset=instr_seg_endo_images, batch_size=5)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=instr_seg_SwinUNETR_model, datamodule=instr_seg_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.645149 seconds\n"
     ]
    }
   ],
   "source": [
    "instr_seg_SwinUNETR_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 40  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(instr_seg_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = instr_seg_SwinUNETR_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
