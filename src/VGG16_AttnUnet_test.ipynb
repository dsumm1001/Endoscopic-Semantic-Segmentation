{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsumm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ignite\\handlers\\checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from monai.networks.nets import BasicUNet\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric, MeanIoU, HausdorffDistanceMetric, ConfusionMatrixMetric\n",
    "from monai.transforms import (\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    ToTensord,\n",
    "    RandFlipd,\n",
    "    RandZoomd, \n",
    "    ToTensord, \n",
    "    AsDiscreted,\n",
    "    CenterSpatialCropd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for pytorch compatibility\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class EndoVis2017Dataset(Dataset):\n",
    "    def __init__(self, label_subdir=None, test=False):\n",
    "        self.data = []\n",
    "\n",
    "        if label_subdir is None:\n",
    "            raise ValueError(\"You must specify a `label_subdir` for ground truth masks (e.g., 'instrument_seg_composite').\")\n",
    "\n",
    "        self.root_dir = \"C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/dataset/test/\"\n",
    "        self.label_subdir = label_subdir\n",
    "\n",
    "        # Recursively walk through directory to find left frame image paths and GT image paths\n",
    "        for subdir, dirs, files in os.walk(self.root_dir):\n",
    "            if 'left_frames' in subdir:\n",
    "                #print(\"Hit!\")\n",
    "                for file in sorted(files):\n",
    "                    if file.endswith(('.png', '.jpg', '.jpeg')):                     \n",
    "                        img_path = os.path.join(subdir, file)\n",
    "                        #print(img_path)\n",
    "\n",
    "                        gt_root = subdir.replace('left_frames', 'ground_truth')\n",
    "                        mask_path = os.path.join(gt_root, self.label_subdir, file)\n",
    "\n",
    "                        if os.path.exists(mask_path):\n",
    "                            #print(\"Hit!\")\n",
    "                            self.data.append({\"image\": img_path, \"label\": mask_path})    # Dictionary for MONAI compatability\n",
    "\n",
    "        if not test:\n",
    "            transforms_list = [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),                        # Loads image data and metadata from file path dictionaries\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),               # Adjust or add the channel dimension of input data to ensure channel_first shape\n",
    "\n",
    "                # Images are of nominal size 1280x1024 --> resizing for memory efficiency\n",
    "                CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(1024, 1280)),         # Cropping background padding from images\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256, 320)),                  # Imported images are of various sizes: standardize to 320,256\n",
    "\n",
    "                # Apply data augmentation techniqes\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.3, spatial_axis=1),               # Horizontal axis flip imposed w/ 30% prob\n",
    "                #RandRotate90d(keys=[\"image\", \"label\"], prob=0.3, max_k=3),                  # Random 90Â° rotation imposed w/ 30% prob\n",
    "                RandZoomd(keys=[\"image\", \"label\"], prob=0.3, min_zoom=0.75, max_zoom=1.25), # Zoom range (+/-25%) imposed w/ 30% prob\n",
    "                #RandAdjustContrastd(keys=[\"image\"], prob=0.3, gamma=(0.75, 1.25)),          # Contrast variation (+/-25%) imposed w/ 30% prob\n",
    "\n",
    "                ScaleIntensityd(keys=[\"image\"]),                            # Scale the intensity of input image to the value range 0-1\n",
    "                ToTensord(keys=[\"image\", \"label\"]),                         # Ensure data is of tensor type for pytorch usage\n",
    "            ]\n",
    "        else:\n",
    "            transforms_list = [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),                        # Loads image data and metadata from file path dictionaries\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),               # Adjust or add the channel dimension of input data to ensure channel_first shape\n",
    "\n",
    "                # Images are of nominal size 1280x1024 --> resizing for memory efficiency\n",
    "                CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(1024, 1280)),         # Cropping background padding from images\n",
    "                Resized(keys=[\"image\", \"label\"], spatial_size=(256, 320)),                  # Imported images are of various sizes: standardize to 320,256\n",
    "\n",
    "                ScaleIntensityd(keys=[\"image\"]),                            # Scale the intensity of input image to the value range 0-1\n",
    "                ToTensord(keys=[\"image\", \"label\"]),                         # Ensure data is of tensor type for pytorch usage\n",
    "            ]\n",
    "        # Additional conditional transforms based on label_subdir\n",
    "        if label_subdir == \"binary_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], threshold=0.5))         # Binary threshold for binary seg\n",
    "        elif label_subdir == \"part_seg_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=5))           # 5 individual class labels for instrument independent part seg\n",
    "        elif label_subdir == \"TypeSegmentation\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=8))            # 8 individual class labels for part independent instrument seg\n",
    "        elif label_subdir == \"instrument_part_seg_composite\":\n",
    "            transforms_list.append(AsDiscreted(keys=[\"label\"], to_onehot=21))           # 26 individual class labels for instrument & part seg\n",
    "\n",
    "        # Imposing MONAI transforms\n",
    "        # https://docs.monai.io/en/stable/transforms.html\n",
    "        self.transform = Compose(transforms_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns number of imported samples\n",
    "        length = len(self.data)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return transformed sample from the dataset as dictated by the index\n",
    "        sample = self.data[idx]\n",
    "        return self.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MONAIDataLoader(LightningDataModule):\n",
    "    def __init__(self, dataset=None, batch_size: int = None, img_size: int = None, dimensions:int = None):\n",
    "        super().__init__()\n",
    "        if dataset is None:\n",
    "            raise ValueError(\"No dataset given!\")\n",
    "        self.dataset = dataset\n",
    "        self.test_dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = True\n",
    "\n",
    "        self.train, self.val = random_split(self.dataset, [\n",
    "            int(len(self.dataset) * 0.8),\n",
    "            len(self.dataset) - int(len(self.dataset) * 0.8)\n",
    "        ])\n",
    "\n",
    "        print(f\"Train dataset size: {len(self.train)}\")\n",
    "        print(f\"Validation dataset size: {len(self.val)}\")\n",
    "        print(f\"Test dataset size: {len(self.test_dataset)}\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # required by PyTorch Lightning\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        #https://docs.pytorch.org/vision/0.12/generated/torchvision.models.vgg16.html\n",
    "        vgg16_feats = models.vgg16(pretrained=pretrained).features\n",
    "        print(\"Length of features in VGG16:\", len(vgg16_feats))\n",
    "\n",
    "        # Divide the layers based on VGG16 architecture\n",
    "        self.encode1 = nn.Sequential(\n",
    "            *vgg16_feats[:5],               # Conv1_1 to MaxPool1, 2 convs (64) + 2 relus + pool\n",
    "            nn.Dropout2d(p=dropout_prob)\n",
    "        )\n",
    "        self.encode2 = nn.Sequential(\n",
    "            *vgg16_feats[5:10],             # Conv2_1 to MaxPool2, 2 convs (128) + 2 relus + pool\n",
    "            nn.Dropout2d(p=dropout_prob)\n",
    "        )\n",
    "        self.encode3 = nn.Sequential(\n",
    "            *vgg16_feats[10:17],            # Conv3_1 to MaxPool3, 3 convs (256) + 3 relus + pool\n",
    "            nn.Dropout2d(p=dropout_prob)\n",
    "        )\n",
    "        self.encode4 = nn.Sequential(\n",
    "            *vgg16_feats[17:24],            # Conv4_1 to MaxPool4, 3 convs (512) + 3 relus + pool\n",
    "            nn.Dropout2d(p=dropout_prob)\n",
    "        )\n",
    "        self.encode5 = nn.Sequential(\n",
    "            *vgg16_feats[24:31],            # Conv5_1 to MaxPool5, 3 convs (512) + 3 relus + pool\n",
    "            nn.Dropout2d(p=dropout_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        layer1 = self.encode1(input)\n",
    "        layer2 = self.encode2(layer1)\n",
    "        layer3 = self.encode3(layer2)\n",
    "        layer4 = self.encode4(layer3)\n",
    "        layer5 = self.encode5(layer4)\n",
    "        return layer1, layer2, layer3, layer4, layer5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels, use_skip=True):\n",
    "        super().__init__()\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        if self.use_skip:\n",
    "            conv_in = out_channels + skip_channels\n",
    "        else:\n",
    "            conv_in = out_channels\n",
    "\n",
    "        #https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3/\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.convblock = nn.Sequential(\n",
    "            nn.Conv2d(conv_in, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, skip=True):\n",
    "        input = self.upconv(input)\n",
    "        if self.use_skip:\n",
    "            input = torch.cat([input, skip], dim=1)\n",
    "        return self.convblock(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using attention block from here\n",
    "#https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.10/paddleseg/models/attention_unet.py#L102\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "\n",
    "        #F_g is gating signal channels\n",
    "        #F_1 is skip connection channels\n",
    "        #F_int is intermediate channels\n",
    "\n",
    "        # Gating signal --> intermediate space\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # Encoder feature map via skip --> intermediate space\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # Reduces combined activation map to single channel mask (1) w/ sigmoid activation\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Learnable upconvolution layer to upsample `g1`\n",
    "        self.upconv = nn.ConvTranspose2d(F_int, F_int, kernel_size=2, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):            # g is decoder feature map, x is encoder skip cxn\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "\n",
    "        # Apply upconv to g1 to match the size of x1\n",
    "        if g1.shape[2:] != x1.shape[2:]:\n",
    "            g1 = self.upconv(g1)\n",
    "\n",
    "        psi = self.relu(g1 + x1)    # sum in intermediate space\n",
    "        psi = self.psi(psi)         # pass through attn mechanism\n",
    "        return x * psi              # apply attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_AttnUNet(LightningModule):\n",
    "    def __init__(self, img_size=(1, 3, 256, 320), batch_size=1, lr=0.001, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        self.example_input_array = [torch.zeros(self.hparams.img_size)]\n",
    "\n",
    "        self.test_step_outputs = []  # Initialize an empty list to store outputs\n",
    "\n",
    "        # Bottleneck conv layers to refine features\n",
    "        # Taken from https://github.com/usuyama/pytorch-unet\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  #in_channels, out_channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.3), \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "\n",
    "        self.encoder = VGGEncoder()\n",
    "        self.decode_4 = Decoder(512, 512, 512)  # 512 -> 512\n",
    "        self.decode_3 = Decoder(512, 256, 256)  # 512 -> 256\n",
    "        self.decode_2 = Decoder(256, 128, 128)  # 256 -> 128\n",
    "        self.decode_1 = Decoder(128, 64, 64)    # 128 -> 64\n",
    "        self.decode_out = Decoder(64, 0, 64, use_skip=False)   # 64 -> 64\n",
    "\n",
    "        self.attn_4 = AttentionBlock(F_g=512, F_l=512, F_int=256)  # for e4\n",
    "        self.attn_3 = AttentionBlock(F_g=512, F_l=256, F_int=128)  # for e3\n",
    "        self.attn_2 = AttentionBlock(F_g=256, F_l=128, F_int=64)   # for e2\n",
    "        self.attn_1 = AttentionBlock(F_g=128, F_l=64,  F_int=32)   # for e1\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "        # Using combined DICE and CE loss as loss function\n",
    "        # Conditional loss function based on the number of classes\n",
    "        if num_classes == 1:\n",
    "            self.DICE_CE_Loss = DiceCELoss(\n",
    "                include_background=False,  # Exclude background class\n",
    "                sigmoid=True,  # Use softmax for multiclass segmentation\n",
    "                softmax=False,  # Apply softmax for multiclass\n",
    "                lambda_dice=1.0,  # Adjust the weight for Dice loss\n",
    "                lambda_ce=1.0,  # Adjust the weight for Cross-Entropy loss\n",
    "                reduction='mean'  # Use mean reduction\n",
    "            )\n",
    "        else:\n",
    "            self.DICE_CE_Loss = DiceCELoss(\n",
    "                include_background=False,  # Exclude background class\n",
    "                sigmoid=False,  # Use softmax for multiclass segmentation\n",
    "                softmax=True,  # Apply softmax for multiclass\n",
    "                lambda_dice=1.0,  # Adjust the weight for Dice loss\n",
    "                lambda_ce=1.0,  # Adjust the weight for Cross-Entropy loss\n",
    "                reduction='mean'  # Use mean reduction\n",
    "            )\n",
    "\n",
    "        # Metric tracking\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\", ignore_empty=True)\n",
    "        self.iou_metric = MeanIoU(include_background=True, reduction=\"mean\", ignore_empty=True)\n",
    "        self.hausdorff_metric = HausdorffDistanceMetric(\n",
    "                                                include_background=True,\n",
    "                                                distance_metric=\"euclidean\",\n",
    "                                                percentile=95,\n",
    "                                                directed=False,\n",
    "                                                reduction=\"mean\"\n",
    "                                            )\n",
    "        self.confusion_metric = ConfusionMatrixMetric(\n",
    "            metric_name=[\"precision\", \"recall\", \"f1 score\"],\n",
    "            include_background=False,\n",
    "            compute_sample=False,\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "        self.dice_scores = []\n",
    "        self.iou_scores = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.last_image = []\n",
    "        self.last_pred = []\n",
    "        self.last_mask = []\n",
    "        self.logged_epochs = []\n",
    "\n",
    "    # Passes model inputs through U-net to get output predictions\n",
    "    def forward(self, inputs):\n",
    "        #print(f\"Input: {inputs.shape}\")\n",
    "\n",
    "        # Spatial flow\n",
    "        # Input:       (1, 3,   256, 320)\n",
    "        # encode1 â    (1, 64,  128, 160)\n",
    "        # encode2 â    (1, 128, 64,  80)\n",
    "        # encode3 â    (1, 256, 32,  40)\n",
    "        # encode4 â    (1, 512, 16,  20)\n",
    "        # encode5 â    (1, 512, 8,   10)\n",
    "        # bottleneck â (1, 512, 8,   10)\n",
    "        # decode_4 â   (1, 512, 16,  20)\n",
    "        # decode_3 â   (1, 256, 32,  40)\n",
    "        # decode_2 â   (1, 128, 64,  80)\n",
    "        # decode_1 â   (1, 64,  128, 160)\n",
    "        # decode_out â (1, 64,  256, 320)\n",
    "        # final_conv â (1, 1,   256, 320)\n",
    "\n",
    "        # Encoder\n",
    "        e1, e2, e3, e4, e5 = self.encoder(inputs)\n",
    "        # print(f\"Encode1: {e1.shape}\")\n",
    "        # print(f\"Encode2: {e2.shape}\")\n",
    "        # print(f\"Encode3: {e3.shape}\")\n",
    "        # print(f\"Encode4: {e4.shape}\")\n",
    "        # print(f\"Encode5: {e5.shape}\")\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(e5)\n",
    "        #print(f\"Bottleneck: {bottleneck_out.shape}\")\n",
    "\n",
    "        # Decoder\n",
    "        e4_attn = self.attn_4(bottleneck_out, e4)\n",
    "        d5 = self.decode_4(bottleneck_out, e4_attn)\n",
    "        #print(f\"Decode4: {d5.shape}\")\n",
    "        e3_attn = self.attn_3(d5, e3)\n",
    "        d4 = self.decode_3(d5, e3_attn)\n",
    "        #print(f\"Decode3: {d4.shape}\")\n",
    "        e2_attn = self.attn_2(d4, e2)\n",
    "        d3 = self.decode_2(d4, e2_attn)\n",
    "        #print(f\"Decode2: {d3.shape}\")\n",
    "        e1_attn = self.attn_1(d3, e1)\n",
    "        d2 = self.decode_1(d3, e1_attn)\n",
    "        #print(f\"Decode1: {d2.shape}\")\n",
    "        d1 = self.decode_out(d2, e1)\n",
    "        #print(f\"DecodeOut: {d1.shape}\")\n",
    "\n",
    "        outputs = self.final_conv(d1)\n",
    "        #print(f\"Final Output: {outputs.shape}\")\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Prepare input and ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        if self.hparams.num_classes == 1:\n",
    "            # Binary segmentation\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            gt_input = (gt_input > 0.5).float()\n",
    "\n",
    "        else:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.nn.functional.one_hot(torch.argmax(probs, dim=1), num_classes=self.num_classes)\n",
    "            preds = preds.permute(0, 3, 1, 2).float()  # Shape: [B, C, H, W]\n",
    "\n",
    "        # MONAI metrics\n",
    "        self.dice_metric(y_pred=preds, y=gt_input)\n",
    "        self.iou_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        # Hausdorff: safe only per image if non-empty\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_i = preds[i]\n",
    "            gt_i = gt_input[i]\n",
    "            if torch.any(pred_i) and torch.any(gt_i):  # Check both non-empty\n",
    "                self.hausdorff_metric(y_pred=pred_i.unsqueeze(0), y=gt_i.unsqueeze(0))\n",
    "            else:\n",
    "                print(f\"[Info] Skipping HD metric for empty prediction or GT in batch index {i}\")\n",
    "        #self.hausdorff_metric(y_pred=preds, y=gt_input)\n",
    "        self.confusion_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        # Extract Dice, IoU, Hausdorff from MONAI\n",
    "        # Aggregate & safely handle NaNs\n",
    "        dice = torch.nan_to_num(self.dice_metric.aggregate(), nan=0.0).item()\n",
    "        iou = torch.nan_to_num(self.iou_metric.aggregate(), nan=0.0).item()\n",
    "        hausdorff = torch.nan_to_num(self.hausdorff_metric.aggregate(), nan=0.0).item()\n",
    "        #hausdorff = self.hausdorff_metric.aggregate().item()\n",
    "        #hausdorff = float('nan') if torch.isnan(torch.tensor(hausdorff)) else hausdorff\n",
    "        #hausdorff = torch.nan_to_num(hausdorff, nan=0.0)\n",
    "\n",
    "        self.dice_metric.reset()\n",
    "        self.iou_metric.reset()\n",
    "        self.hausdorff_metric.reset()\n",
    "\n",
    "        # Extract precision, recall, f1 score\n",
    "        confusion_metrics = self.confusion_metric.aggregate()\n",
    "        precision, recall, f1 = [m.item() for m in confusion_metrics]\n",
    "        self.confusion_metric.reset()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"test_dice\", dice, prog_bar=True)\n",
    "        self.log(\"test_iou\", iou, prog_bar=True)\n",
    "        self.log(\"test_hausdorff\", hausdorff, prog_bar=True)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "        self.log(\"test_f1\", f1, prog_bar=True)\n",
    "\n",
    "        # Return for aggregation\n",
    "        out = {\n",
    "            \"test_dice\": torch.tensor(dice),\n",
    "            \"test_iou\": torch.tensor(iou),\n",
    "            \"test_precision\": torch.tensor(precision),\n",
    "            \"test_recall\": torch.tensor(recall),\n",
    "            \"test_f1\": torch.tensor(f1),\n",
    "            \"test_hausdorff\": torch.tensor(hausdorff)\n",
    "        }\n",
    "\n",
    "        self.test_step_outputs.append(out)\n",
    "        return out\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Aggregate the results across all batches in the epoch\n",
    "        avg_dice = torch.stack([x[\"test_dice\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_iou = torch.stack([x[\"test_iou\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_hausdorff = torch.stack([x[\"test_hausdorff\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_precision = torch.stack([x[\"test_precision\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_recall = torch.stack([x[\"test_recall\"] for x in self.test_step_outputs]).mean()\n",
    "        avg_f1 = torch.stack([x[\"test_f1\"] for x in self.test_step_outputs]).mean()\n",
    "\n",
    "        print(f\"\\nâ Test Metrics:\"\n",
    "            f\"\\n   Dice       : {avg_dice.item():.4f}\"\n",
    "            f\"\\n   IoU        : {avg_iou.item():.4f}\"\n",
    "            f\"\\n   Hausdorff  : {avg_hausdorff.item():.4f}\"\n",
    "            f\"\\n   Precision  : {avg_precision.item():.4f}\"\n",
    "            f\"\\n   Recall     : {avg_recall.item():.4f}\"\n",
    "            f\"\\n   F1 Score   : {avg_f1.item():.4f}\")\n",
    "\n",
    "        # Clear for next epoch\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Gets labels for input and corresponding ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "\n",
    "        # Call forward pass\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        # Compute DICE & CE loss based on current params\n",
    "        loss = self.DICE_CE_Loss(outputs, gt_input)\n",
    "\n",
    "        # Log DICE loss with PyTorch Lightning logger\n",
    "        self.log(f\"Train_Dice_CE_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Append train loss at the end of each epoch\n",
    "        if batch_idx == len(batch) - 1:\n",
    "            self.train_losses.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        # Gets labels for input and corresponding ground truth\n",
    "        inputs, gt_input = self._prepare_batch(batch)\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.DICE_CE_Loss(outputs, gt_input)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        if self.hparams.num_classes == 1:\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            # Ensure ground truth is binary (i.e., 0 or 1)\n",
    "            gt_input = (gt_input > 0.5).float()  # Threshold the ground truth if needed\n",
    "\n",
    "            intersection = (preds * gt_input).sum()\n",
    "            union = preds.sum() + gt_input.sum()\n",
    "            bin_dice_score = 2.0 * intersection / (union + 1e-8)  # Avoid division by zero\n",
    "            # IoU score calculation for binary segmentation\n",
    "            bin_iou_score = intersection / (union - intersection + 1e-8)  # Avoid division by zero\n",
    "\n",
    "            self.log(\"val_dice\", bin_dice_score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.log(\"val_iou\", bin_iou_score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        else:\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.nn.functional.one_hot(torch.argmax(probs, dim=1), num_classes=self.num_classes)\n",
    "            preds = preds.permute(0, 3, 1, 2).float()  # Shape: [B, C, H, W]\n",
    "\n",
    "            self.dice_metric(y_pred=preds, y=gt_input)\n",
    "            self.iou_metric(y_pred=preds, y=gt_input)\n",
    "\n",
    "        if self.trainer.sanity_checking:\n",
    "            return  # skip logging during sanity check\n",
    "\n",
    "        # Append validation loss at the end of each epoch\n",
    "        if batch_idx == len(batch) - 1:\n",
    "            self.val_losses.append(loss.item())\n",
    "\n",
    "            # For binary segmentation: apply sigmoid and threshold\n",
    "            if self.hparams.num_classes == 1:\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                outputs = (outputs > 0.5).float()  # Convert probabilities to binary mask\n",
    "                self.dice_scores.append(round(bin_dice_score.item(), 4))\n",
    "                print(\"Dice\", bin_dice_score)\n",
    "                self.iou_scores.append(round(bin_iou_score.item(), 4))\n",
    "                print(\"IOU\", bin_iou_score)\n",
    "\n",
    "            # For multiclass segmentation: apply softmax\n",
    "            else:\n",
    "                outputs = torch.softmax(outputs, dim=1)  # Apply softmax for multi-class outputs\n",
    "                dice = self.dice_metric.aggregate()[0].item()\n",
    "                print(\"Dice\", round(dice, 4))\n",
    "                iou = self.iou_metric.aggregate()[0].item()\n",
    "                print(\"IOU\", round(iou, 4))\n",
    "                self.dice_metric.reset()\n",
    "                self.iou_metric.reset()\n",
    "                self.dice_scores.append(dice)\n",
    "                self.iou_scores.append(iou)\n",
    "                self.log(\"val_dice\", dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                self.log(\"val_iou\", iou, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            # Normalize and convert tensor to 3 channels (RGB) for visualization\n",
    "            def process(last):\n",
    "                # Detach from cpu to not interrupt training\n",
    "                # https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor\n",
    "                last = last[0].detach().cpu()\n",
    "\n",
    "                # Min max normalization\n",
    "                # https://www.codecademy.com/article/normalization\n",
    "                last= (last - last.min()) / (last.max() - last.min() + 1e-8)\n",
    "\n",
    "                # If grayscale, reshape last image to RGB for display by replicating gray value twice\n",
    "                # https://discuss.pytorch.org/t/convert-grayscale-images-to-rgb/113422\n",
    "                return last.repeat(3, 1, 1) if last.shape[0] == 1 else last\n",
    "\n",
    "            current_epoch = self.current_epoch\n",
    "            total_epochs = self.trainer.max_epochs\n",
    "            #print(\"TE\", total_epochs)\n",
    "\n",
    "            if current_epoch == 0 or current_epoch == total_epochs - 1 or current_epoch == total_epochs // 2:\n",
    "                self.last_image.append(process(inputs))\n",
    "                self.last_pred.append(process(outputs))\n",
    "                self.last_mask.append(process(gt_input))\n",
    "                self.logged_epochs.append(current_epoch)\n",
    "                print(f\"Logged image from epoch {current_epoch}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "    #    return self(batch['image'])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #set optimizer\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # halve LR every 5 epochs\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _prepare_batch(self, batch):\n",
    "        return batch['image'], batch['label']\n",
    "\n",
    "    # Plot training and val losses when needed\n",
    "    def plot_losses(self):\n",
    "        min_len = min(len(self.train_losses), len(self.val_losses))\n",
    "        epochs = range(1, min_len + 1)\n",
    "\n",
    "        # Plotting training vs validation loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, self.train_losses[:len(epochs)], label=\"Training Loss\", color='blue')\n",
    "        plt.plot(epochs, self.val_losses[:len(epochs)], label=\"Validation Loss\", color='orange')\n",
    "        plt.title(\"Training vs Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.dice_scores) + 1)\n",
    "\n",
    "        # Convert to CPU floats if necessary\n",
    "        dice = [d.cpu().item() if torch.is_tensor(d) else d for d in self.dice_scores]\n",
    "        iou = [i.cpu().item() if torch.is_tensor(i) else i for i in self.iou_scores]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, dice, label='Dice Coefficient')\n",
    "        plt.plot(epochs, iou, label='IoU')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(\"Validation Metrics Over Time\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_result_by_epoch(self):\n",
    "        total_epochs = len(self.last_image)\n",
    "\n",
    "        if total_epochs < 5:\n",
    "            print(f\"Only {total_epochs} epochs recorded, plotting all.\")\n",
    "            selected_epochs = list(range(total_epochs))\n",
    "        else:\n",
    "            print(f\"{total_epochs} epochs recorded, bug in code.\")\n",
    "\n",
    "        for epoch_idx in selected_epochs:\n",
    "            epoch_num = self.logged_epochs[epoch_idx] if hasattr(self, \"logged_epochs\") else epoch_idx\n",
    "            img = self.last_image[epoch_idx]\n",
    "            pred = self.last_pred[epoch_idx]\n",
    "            mask = self.last_mask[epoch_idx]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "            ax[0].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "            ax[0].set_title(f\"Epoch {epoch_num} - Image\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            if self.hparams.num_classes == 1:\n",
    "                ax[1].imshow(np.transpose(pred.numpy(), (1, 2, 0)))\n",
    "                ax[1].set_title(f\"Epoch {epoch_num} - Prediction\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                ax[2].imshow(np.transpose(mask.numpy(), (1, 2, 0)))\n",
    "                ax[2].set_title(f\"Epoch {epoch_num} - Ground Truth\")\n",
    "                ax[2].axis(\"off\")\n",
    "            else:\n",
    "                # Define the colormap and normalization\n",
    "                num_classes = self.hparams.num_classes\n",
    "                cmap = plt.get_cmap('viridis', num_classes)\n",
    "                bounds = np.arange(num_classes + 1) - 0.5\n",
    "                norm = plt.matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "                # Convert one-hot encoded predictions and masks to single-channel class labels\n",
    "                pred_mask = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "                true_mask = torch.argmax(mask, dim=0).cpu().numpy()\n",
    "\n",
    "                # Apply consistent colormap and normalization\n",
    "                im1 = ax[1].imshow(pred_mask, cmap=cmap, norm=norm)\n",
    "                ax[1].set_title(f\"Epoch {epoch_num} - Prediction\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                im2 = ax[2].imshow(true_mask, cmap=cmap, norm=norm)\n",
    "                ax[2].set_title(f\"Epoch {epoch_num} - Ground Truth\")\n",
    "                ax[2].axis(\"off\")\n",
    "\n",
    "                im_for_cbar = im1  # just need one mappable\n",
    "\n",
    "                # Adjust layout to leave space at the bottom\n",
    "                fig.subplots_adjust(bottom=0.25) # tweak this if labels get cut off\n",
    "\n",
    "                # Add a new axis below the plots for the colorbar\n",
    "                cbar_ax = fig.add_axes([0.1, 0.1, 0.8, 0.10])  # [left, bottom, width, height]\n",
    "                cbar = fig.colorbar(im_for_cbar, cax=cbar_ax, orientation='horizontal', ticks=np.arange(num_classes))\n",
    "\n",
    "                # Add colorbar below the plots\n",
    "                #cbar = fig.colorbar(im1, ax=ax.ravel().tolist(), orientation='horizontal',\n",
    "                        #ticks=np.arange(num_classes), pad=0.15, fraction=0.05)\n",
    "\n",
    "                # Set class labels\n",
    "                if num_classes == 5:\n",
    "                    cbar.ax.set_xticklabels(['Background', 'Shaft', 'Wrist', 'Claspers', 'Probe'])\n",
    "                elif num_classes == 8:\n",
    "                    cbar.ax.set_xticklabels(['Background', 'Bipolar Forceps', 'Prograsp Forceps', 'Large Needle Driver',\n",
    "                                            'Vessel Sealer', 'Grasping Retractor', 'Monopolar Curved Scissors', 'Other'])\n",
    "\n",
    "                    plt.setp(cbar.ax.get_xticklabels(), rotation=30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "                elif num_classes == 21:\n",
    "                    cbar.ax.set_xticklabels([\n",
    "                        \"Background\",\n",
    "                        \"Bipolar Forceps Shaft\", \"Bipolar Forceps Wrist\", \"Bipolar Forceps Claspers\",\n",
    "                        \"Prograsp Forceps Shaft\", \"Prograsp Forceps Wrist\", \"Prograsp Forceps Claspers\",\n",
    "                        \"Large Needle Driver Shaft\", \"Large Needle Driver Wrist\", \"Large Needle Driver Claspers\",\n",
    "                        \"Vessel Sealer Shaft\", \"Vessel Sealer Wrist\", \"Vessel Sealer Claspers\",\n",
    "                        \"Grasping Retractor Shaft\", \"Grasping Retractor Wrist\", \"Grasping Retractor Claspers\",\n",
    "                        \"Monopolar Curved Scissors Shaft\", \"Monopolar Curved Scissors Wrist\", \"Monopolar Curved Scissors Claspers\",\n",
    "                        \"Other Probe\",\"Other Probe\"\n",
    "                    ])\n",
    "                    plt.setp(cbar.ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        \n",
    "                cbar.set_label('Class ID')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of features in VGG16: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39744291d024c0e813a7bc8594ca111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Skipping HD metric for empty prediction or GT in batch index 12\n",
      "\n",
      "â Test Metrics:\n",
      "   Dice       : 0.9160\n",
      "   IoU        : 0.8650\n",
      "   Hausdorff  : 26.0808\n",
      "   Precision  : 0.8957\n",
      "   Recall     : 0.9591\n",
      "   F1 Score   : 0.9172\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\">        Test metric        </span>â<span style=\"font-weight: bold\">       DataLoader 0        </span>â\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.9159815907478333     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.9172326922416687     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    26.080799102783203     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.8650036454200745     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.8957014679908752     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.9591355919837952     </span>â\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.9159815907478333    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.9172326922416687    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   26.080799102783203    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.8650036454200745    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.8957014679908752    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.9591355919837952    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.9159815907478333,\n",
       "  'test_iou': 0.8650036454200745,\n",
       "  'test_hausdorff': 26.080799102783203,\n",
       "  'test_precision': 0.8957014679908752,\n",
       "  'test_recall': 0.9591355919837952,\n",
       "  'test_f1': 0.9172326922416687}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_vgg16_AttnUNet_model = VGG16_AttnUNet(num_classes=1)\n",
    "binary_vgg16_AttnUNet_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/vgg16_AttnUNet/binary_vgg16_AttnUNet_model.pth'))\n",
    "\n",
    "binary_endo_images = EndoVis2017Dataset(label_subdir='binarySegmentation', test=True)\n",
    "binary_endo_data = MONAIDataLoader(dataset=binary_endo_images, batch_size=20)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=binary_vgg16_AttnUNet_model, datamodule=binary_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.011399 seconds\n"
     ]
    }
   ],
   "source": [
    "binary_vgg16_AttnUNet_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 10  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(binary_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = binary_vgg16_AttnUNet_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of features in VGG16: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faaf41978de466e8375ead248fca514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the ground truth of class 4 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 4 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 2 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 1 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 2 is all 0, this may result in nan/inf distance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â Test Metrics:\n",
      "   Dice       : 0.7977\n",
      "   IoU        : 0.7135\n",
      "   Hausdorff  : 43.0073\n",
      "   Precision  : 0.7786\n",
      "   Recall     : 0.8153\n",
      "   F1 Score   : 0.7889\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\">        Test metric        </span>â<span style=\"font-weight: bold\">       DataLoader 0        </span>â\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.7977108359336853     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.7889207601547241     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">     43.00724411010742     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.7135457992553711     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.7785787582397461     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.8153424263000488     </span>â\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.7977108359336853    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.7889207601547241    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m    43.00724411010742    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.7135457992553711    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.7785787582397461    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.8153424263000488    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.7977108359336853,\n",
       "  'test_iou': 0.7135457992553711,\n",
       "  'test_hausdorff': 43.00724411010742,\n",
       "  'test_precision': 0.7785787582397461,\n",
       "  'test_recall': 0.8153424263000488,\n",
       "  'test_f1': 0.7889207601547241}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_seg_vgg16_AttnUNet_model = VGG16_AttnUNet(num_classes=5)\n",
    "part_seg_vgg16_AttnUNet_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/vgg16_AttnUNet/part_seg_vgg16_AttnUNet_model.pth'))\n",
    "\n",
    "part_seg_endo_images = EndoVis2017Dataset(label_subdir='part_seg_composite', test=True)\n",
    "part_seg_endo_data = MONAIDataLoader(dataset=part_seg_endo_images, batch_size=10)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=part_seg_vgg16_AttnUNet_model, datamodule=part_seg_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.451475 seconds\n"
     ]
    }
   ],
   "source": [
    "part_seg_vgg16_AttnUNet_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 20  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(part_seg_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = part_seg_vgg16_AttnUNet_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of features in VGG16: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 720\n",
      "Validation dataset size: 180\n",
      "Test dataset size: 900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932438701d84496592e0f26e92953166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the ground truth of class 5 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 6 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 7 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 5 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 6 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 7 is all 0, this may result in nan/inf distance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â Test Metrics:\n",
      "   Dice       : 0.5103\n",
      "   IoU        : 0.4586\n",
      "   Hausdorff  : 53.2915\n",
      "   Precision  : 0.6138\n",
      "   Recall     : 0.6235\n",
      "   F1 Score   : 0.6130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\">        Test metric        </span>â<span style=\"font-weight: bold\">       DataLoader 0        </span>â\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_dice         </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.5102523565292358     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.6129774451255798     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_hausdorff       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">     53.29149627685547     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.4585757255554199     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.6138267517089844     </span>â\n",
       "â<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â<span style=\"color: #800080; text-decoration-color: #800080\">    0.6234978437423706     </span>â\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_dice        \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.5102523565292358    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.6129774451255798    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_hausdorff      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m    53.29149627685547    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.4585757255554199    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.6138267517089844    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "â\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ\u001b[35m \u001b[0m\u001b[35m   0.6234978437423706    \u001b[0m\u001b[35m \u001b[0mâ\n",
       "âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_dice': 0.5102523565292358,\n",
       "  'test_iou': 0.4585757255554199,\n",
       "  'test_hausdorff': 53.29149627685547,\n",
       "  'test_precision': 0.6138267517089844,\n",
       "  'test_recall': 0.6234978437423706,\n",
       "  'test_f1': 0.6129774451255798}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_seg_vgg16_AttnUNet_model = VGG16_AttnUNet(num_classes=8)\n",
    "instr_seg_vgg16_AttnUNet_model.load_state_dict(torch.load('C:/Users/dsumm/OneDrive/Documents/UMD ENPM Robotics Files/BIOE658B (Intro to Medical Image Analysis)/Project/results/vgg16_AttnUNet/instr_seg_vgg16_AttnUNet_model.pth'))\n",
    "\n",
    "instr_seg_endo_images = EndoVis2017Dataset(label_subdir='TypeSegmentation', test=True)\n",
    "instr_seg_endo_data = MONAIDataLoader(dataset=instr_seg_endo_images, batch_size=5)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.test(model=instr_seg_vgg16_AttnUNet_model, datamodule=instr_seg_endo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image over 200 images: 0.346484 seconds\n"
     ]
    }
   ],
   "source": [
    "instr_seg_vgg16_AttnUNet_model.eval().cuda()  # <<< This is important\n",
    "N_BATCHES = 40  # Set number of batches to evaluate\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(instr_seg_endo_data.test_dataloader()):\n",
    "        if i >= N_BATCHES:\n",
    "            break\n",
    "        inputs = batch[\"image\"].cuda()\n",
    "        start_time = time.time()\n",
    "        outputs = instr_seg_vgg16_AttnUNet_model(inputs)\n",
    "        torch.cuda.synchronize()  # Ensures accurate timing on GPU\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "avg_infer_time = np.mean(times) / inputs.shape[0]  # Per image\n",
    "print(f\"Average inference time per image over {N_BATCHES * inputs.shape[0]} images: {avg_infer_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
